# Neural-Style-Transfer
Computing a Image Style Transfer model by using VGG-19 convolutional neural network.
## Team Info:
Weihan Chu |
[Shuyi Huo](https://github.com/ShuyiHuo) |
Siqi Chen
## Introduction
Our team is trying to implement a high-performance Image Style Transfer algorithm from scratch and apply it to establish a photo style transfer web frontend. For computing a high-performance algorithm, our team utilized a famous image processing pre-trained CNN named VGG-19 and compiled other models for comparison; For output validation, we use different content pictures (portrait, scene etc.) and different style pictures (famous paints from variate artistic genres) to compute our target images; For evaluating model performance, our team combined content loss with style loss and total variation loss for tuning the ideal target images. Since there is no absolutely quantified result on the performance of style transformation, our team made a brief inference and conclusion.
## Reference
[1]Gatys, Leon A., Alexander S. Ecker, and Matthias Bethge. “Image Style Transfer Using Convolutional Neural Networks.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.

[2]Keras-Team, ed. “Keras-Team/Keras.” GitHub, n.d. https://github.com/keras-team/keras/tree/fcf2ed7831185a282895dda193217c2a97e1e41d.

[3]“Neural Style Transfer: TensorFlow Core.” TensorFlow, n.d. https://www.tensorflow.org/tutorials/generative/style_transfer.
